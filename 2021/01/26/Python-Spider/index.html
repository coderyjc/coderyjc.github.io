<!DOCTYPE html>
<html lang="en">
    
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <meta name="description" content="笔记 | Python爬虫" />
    <meta name="hexo-theme-A4" content="v1.9.6" />
    <link rel="alternate icon" type="image/webp" href="/img/avatar.png">
    <title>Jingcun Yan</title>

    
        
<link rel="stylesheet" href="/css/highlight/style1.css">

        
<link rel="stylesheet" href="/css/reset.css">

        
<link rel="stylesheet" href="/css/markdown.css">

        
<link rel="stylesheet" href="/css/fonts.css">
 
         <!--注意：首页既不是post也不是page-->
        
        
        
<link rel="stylesheet" href="/css/ui.css">
 
        
<link rel="stylesheet" href="/css/style.css">


        
            <!--返回顶部css-->
            
<link rel="stylesheet" href="/css/returnToTop.css">

            
<link rel="stylesheet" href="/css/unicons.css">

        
        
            <!--目录-->
            
<link rel="stylesheet" href="/css/toc.css">

        
    

    
        
<link rel="stylesheet" href="/css/returnToLastPage.css">

    
    
   
<link rel="stylesheet" href="/css/lightgallery-bundle.min.css">


<meta name="generator" content="Hexo 7.3.0"></head>
    
    
        <style>
            .index-main{
                max-width:  880px;
            }
        </style>

    
    



    

    
    

    
    
    
    <body>
        <script src="/js/darkmode-js.min.js"></script>
        
        <script>
            const options = {
                bottom: '40px', // default: '32px'
                right: 'unset', // default: '32px'
                left: '42px', // default: 'unset'
                time: '0.3s', // default: '0.3s'
                mixColor: '#fff', // default: '#fff'
                backgroundColor: ' #e4e4e4 ',  // default: '#fff'
                buttonColorDark: '#100f2c',  // default: '#100f2c'
                buttonColorLight: '#fff', // default: '#fff'
                saveInCookies: true, // default: true,
                label: '🌓', // default: ''
                autoMatchOsTheme: true // default: true
            }
            const darkmode = new Darkmode(options);
            darkmode.showWidget();
        </script>
        
        
            <div class="left-toc-container">
                <nav id="toc" class="bs-docs-sidebar"></nav>
            </div>
        
        <div class="paper">
            
            
            
            
                <div class="shadow-drop-2-bottom paper-main">
                    


<div class="header">
    <div class="header-container">
        <img style="
        width: 56px;
        height: auto;" alt="^-^" cache-control="max-age=86400" class="header-img" src="/img/avatar.png" width="10%"></img>
        <div class="header-content">
            <a class="logo" href="/">Jingcun Yan</a> 
            <span class="description"></span> 
        </div>
        
    </div>
    
   
    <ul class="nav">
        
            
                <li><a href="/">首页</a></li>
            
        
            
                <li><a href="/list/">文章</a></li>
            
        
            
                <li><a href="/collect/">收藏</a></li>
            
        
    </ul>
</div> 
        
                    
                    

                    
                    
                    
                    <!--说明是文章post页面-->
                    
                        <div class="post-main">
    

    
        
            
                <div class="post-main-title" style="text-align: center;">
                    笔记 | Python爬虫
                </div>
            
        
      
    

    

        
            <div class="post-head-meta-center">
        
                
                
                    
                     <span>字数总计：8.7k</span>
                
                
                    
                        &nbsp; | &nbsp;
                    
                    <span>阅读估时：35分钟</span>
                
                
            </div>
    

    <div class="post-md">
        
        <div class=".article-gallery"><h1 id="Python爬虫"><a href="#Python爬虫" class="headerlink" title="Python爬虫"></a>Python爬虫</h1><blockquote>
<p><a target="_blank" rel="noopener" href="https://book.apeland.cn/details/66/">学习文档(https://book.apeland.cn/details/66/)</a></p>
</blockquote>
<h2 id="爬虫基础简介"><a href="#爬虫基础简介" class="headerlink" title="爬虫基础简介"></a>爬虫基础简介</h2><p>爬虫在使用场景中的分类</p>
<ul>
<li>通用爬虫：抓取系统重要组成部分。抓取的是一整张页面数据。</li>
<li>聚焦爬虫：是建立在通用爬虫的基础之上。抓取的是页面中特定的局部内容。</li>
<li>增量式爬虫：检测网站中数据更新的情况。只会抓取网站中最新更新出来的数据。</li>
</ul>
<p>robots.txt 规定了哪些数据可以被爬取，哪些不能爬取</p>
<p>http协议：就是服务器和客户端进行数据交互的一种形式。</p>
<p>常用请求头信息</p>
<ul>
<li>User-Agent：请求载体的身份标识</li>
<li>Connection：请求完毕后，是断开连接还是保持连接常用响应头信息</li>
</ul>
<p>常用的响应头消息</p>
<ul>
<li>Content-Type：服务器响应回客户端的数据类型</li>
</ul>
<p>https协议：安全的超文本传输协议 - 传输的文本进行了加密</p>
<p>加密方式：</p>
<ul>
<li>对称密钥加密<ul>
<li>“共享密钥加密”，也就是说，客户端将文件加密之后，将密钥和加密后的信息一起传送给服务端，加密和解密的密钥是同一个，这种方式看起来安全，但是仍然有安全隐患。</li>
</ul>
</li>
<li>非对称密钥加密<ul>
<li>服务端制造密钥对（公钥-&gt;用来加密 和 私钥-&gt;用来解密），将公钥发送给客户端，客户端使用公钥加密文件，将加密后的文件发送给服务端，服务端用私钥进行解密。</li>
<li>技术难题（缺点）：保证接收端向发送端发出公开秘钥的时候，发送端确保收到的是预先要发送的，而不会被挟持。效率低，处理复杂，影响通信速度</li>
</ul>
</li>
<li>证书密钥加密</li>
</ul>
<h2 id="requests模块基础"><a href="#requests模块基础" class="headerlink" title="requests模块基础"></a>requests模块基础</h2><p>步骤</p>
<ul>
<li>指定url - <strong>这个url一定是通过抓包获取的，直接输入地址栏可能不会达到预期的结果</strong></li>
<li>发起请求</li>
<li>获取响应数据</li>
<li>持久化存储</li>
</ul>
<p>爬取搜狗页面的数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 指定url（通过抓包获取）</span></span><br><span class="line">    url = <span class="string">&quot;http://www.kekenet.com/&quot;</span>;</span><br><span class="line">    <span class="comment"># 发起请求</span></span><br><span class="line">    responce = requests.get(url = url);</span><br><span class="line">    <span class="comment"># 设置字符编码</span></span><br><span class="line">    responce.encoding = <span class="string">&#x27;utf-8&#x27;</span></span><br><span class="line">    <span class="comment"># 获取响应数据</span></span><br><span class="line">    page_text = responce.text</span><br><span class="line">    <span class="comment"># 持久化存储</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;baidu.txt&quot;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(page_text);</span><br><span class="line">        fp.close()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Success!&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="UA伪装"><a href="#UA伪装" class="headerlink" title="UA伪装"></a>UA伪装</h3><p>反爬机制</p>
<ul>
<li>User-Agent：请求载体的身份标识，使用浏览器发起的请求，请求载体的身份标识为浏览器，使用爬虫程序发起的请求，请求载体为爬虫程序。</li>
<li>UA检测：相关的门户网站通过检测请求该网站的载体身份来辨别该请求是否为爬虫程序，如果是，则网站数据请求失败。因为正常用户对网站发起的请求的载体一定是基于某一款浏览器，如果网站检测到某一请求载体身份标识不是基于浏览器的，则让其请求失败。因此，UA检测是我们整个课程中遇到的第二种反爬机制，第一种是robots协议</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    url = <span class="string">&#x27;https://www.sogou.com/web&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 请求头，里面设置我们的UA</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Linux; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.66 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 处理url中携带的参数-封装到字典中</span></span><br><span class="line">    kw = <span class="built_in">input</span>(<span class="string">&#x27;enter a keyword :&#x27;</span>)</span><br><span class="line">    <span class="comment"># 请求中的参数</span></span><br><span class="line">    param = &#123;</span><br><span class="line">        <span class="string">&#x27;query&#x27;</span>: kw</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># url是携带参数的，并且请求过程中处理了参数</span></span><br><span class="line">    response = requests.get(url=url, params=param, headers=headers)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果页面显示乱码，可以加上这一句</span></span><br><span class="line">    <span class="comment"># response.encoding = &#x27;utf-8&#x27;</span></span><br><span class="line"></span><br><span class="line">    page_text = response.text</span><br><span class="line">    <span class="comment"># 持久化存储</span></span><br><span class="line">    fileName = kw + <span class="string">&#x27;.html&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fileName, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(page_text)</span><br><span class="line">        fp.close()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;success&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="爬取局部信息（ajax-post）"><a href="#爬取局部信息（ajax-post）" class="headerlink" title="爬取局部信息（ajax-post）"></a>爬取局部信息（ajax-post）</h3><p>我们已经知道，百度翻译的单词翻译区是基于ajax的局部刷新实现的</p>
<p>![[img&#x2F;image-20210126171017846.png]]<br>因此我们可以通过抓包工具来获取发送过来的数据。</p>
<p>![[img&#x2F;image-20210126171319186.png]]<br>从下方数据包中找到我们需要的dog有关的json数据，可以发现，我们发送的表单数据为 kw:”dog”，因此我们的表单需要一个kw属性的数据，值为我们要查找的数据。</p>
<p>![[img&#x2F;image-20210126171500291.png]]<br>查看响应，发现里面封装的json数据刚好是我们需要的。</p>
<p>![[img&#x2F;image-20210126171619847.png]]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 获取用户输入的关键词</span></span><br><span class="line">    keyWord = <span class="built_in">input</span>(<span class="string">&quot;enter a word to search: &quot;</span>)</span><br><span class="line">    <span class="comment"># 设置UA</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&#x27;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 设置url</span></span><br><span class="line">    url = <span class="string">&quot;https://fanyi.baidu.com/sug&quot;</span></span><br><span class="line">    <span class="comment"># 将要发送的数据</span></span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">&quot;kw&quot;</span>: keyWord</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 获取回应的数据</span></span><br><span class="line">    response = requests.post(url=url, data=data, headers=headers)</span><br><span class="line">    <span class="comment"># 将回应的数据转化为json格式</span></span><br><span class="line">    json_data = response.json()</span><br><span class="line">    <span class="comment"># 数据的持久化存储</span></span><br><span class="line">    fileName = keyWord + <span class="string">&#x27;.json&#x27;</span></span><br><span class="line">    fp = <span class="built_in">open</span>(fileName, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    json.dump(json_data, fp, ensure_ascii=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>扩展：json数据格式与python</p>
<p>将python 的数据结构转换为json：<code>json_str = json.dumps(data)</code> 其中，data是一个json格式的字典类型数据; 将json编码的字符串转化为python格式<code>data = json.loads(json_str)</code></p>
<p>如果处理的是文件，可以使用dump与loda编码和解析json数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Writing JSON data</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    json.dump(data, f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reading data back</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = json.load(f)</span><br></pre></td></tr></table></figure>
</blockquote>
<h3 id="爬取局部信息（ajax-get）"><a href="#爬取局部信息（ajax-get）" class="headerlink" title="爬取局部信息（ajax-get）"></a>爬取局部信息（ajax-get）</h3><p>我们来到 <a target="_blank" rel="noopener" href="https://movie.douban.com/typerank?type_name=%E5%8A%A8%E4%BD%9C&type=5&interval_id=100:90&action=">豆瓣动作电影分类排行榜</a> 发现在将页面下拉到最底部的时候会刷新出来由此我们可以判定是ajax动态刷新页面，打开浏览器自带的抓包工具，过滤xhr请求，将页面下拉到最下面之后，我们可以捕获到刷新出来的新的数据的信息。</p>
<p>![[img&#x2F;image-20210126175023398.png]]<br>在下拉到时候我们可以看到出现这么一条信息，消息头是get方式，链接为<code>https://movie.douban.com/j/chart/top_list?type=5&amp;interval_id=100:90&amp;action=&amp;start=20&amp;limit=20</code></p>
<p>因此我们可以查看这个消息头，获取数据格式，我们可以自己设置字典的参数来获取数据</p>
<p>![[img&#x2F;image-20210126175509471.png]]<br>观察这个链接，发现不止有这几个参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 设置UA</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&#x27;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 设置url</span></span><br><span class="line">    url = <span class="string">&quot;https://movie.douban.com/j/chart/top_list&quot;</span></span><br><span class="line">    <span class="comment"># 将要发送的数据</span></span><br><span class="line">    param = &#123;</span><br><span class="line">        <span class="comment"># 注意！！！这里的键值对一定要设置正确，拼写、空格都要正确，不要有多余的空格或者少的空格！！！！</span></span><br><span class="line">        <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;5&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;interval_id&#x27;</span>: <span class="string">&#x27;100:90&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;action&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;start&#x27;</span>: <span class="string">&#x27;0&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;limit&#x27;</span>: <span class="string">&#x27;20&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 获取回应的数据</span></span><br><span class="line">    response = requests.get(url=url, headers=headers, params=param)</span><br><span class="line">    <span class="comment"># 将回应的数据转化为json格式</span></span><br><span class="line">    json_data = response.json()</span><br><span class="line">    <span class="comment"># 数据的持久化存储</span></span><br><span class="line">    fp = <span class="built_in">open</span>(<span class="string">&quot;movieRank&quot;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    json.dump(json_data, fp, ensure_ascii=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h3 id="综合案例"><a href="#综合案例" class="headerlink" title="综合案例"></a>综合案例</h3><p>爬取国家药品监督管理总局中基于中华人民共和国化妆品生产许可证相关数据 <a target="_blank" rel="noopener" href="http://scxk.nmpa.gov.cn:81/xk/">http://scxk.nmpa.gov.cn:81/xk/</a></p>
<p>具体代码查看<code>Learning\Python\Spider\01-UsageOfRequest\06-challenge.py</code></p>
<h2 id="数据解析"><a href="#数据解析" class="headerlink" title="数据解析"></a>数据解析</h2><p>聚焦爬虫：爬取页面中指定页面的内容。在聚焦爬虫中要用到数据解析。</p>
<p>数据解析原理概述：</p>
<ul>
<li>解析的局部的文本内容都会在标签之间或者标签对应的属性中进行存储<ol>
<li>进行指定标签的定位</li>
<li>标签或者标签对应的属性中存储的数据值进行提取（解析）</li>
</ol>
</li>
</ul>
<p>步骤为：</p>
<ul>
<li>指定url</li>
<li>发起请求</li>
<li>获取响应数据</li>
<li>数据解析</li>
<li>持久化存储</li>
</ul>
<h3 id="xpath解析【重点】"><a href="#xpath解析【重点】" class="headerlink" title="xpath解析【重点】"></a>xpath解析【重点】</h3><p>最常用、最便捷、最高效、最通用。</p>
<p>原理：</p>
<ol>
<li>实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中。</li>
<li>调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获。</li>
</ol>
<p>环境安装：</p>
<ul>
<li><code>pip install lxml</code></li>
</ul>
<p>实例化一个etree对象 <code>from lxml import etree</code></p>
<ol>
<li>可以将<strong>本地的html文档</strong>中的源码数据加载到etree对象中：<code>etree.parse(filePath)</code></li>
<li>可以将从<strong>互联网</strong>上获取的源码数据加载到该对象中：<code>etree.HTML(&#39;page_text&#39;)</code></li>
</ol>
<p>调用xpath方法进行解析：<code>xpath(&#39;xpath表达式&#39;)</code> 【重点】</p>
<p>xpath表达式(始终返回一个列表)：</p>
<ul>
<li><code>/</code>表示的是从根节点开始定位。表示的是一个层级 <code>elem = tree.xpath(&#39;/html/body/h1&#39;)</code></li>
<li><code>//</code> 表示可以表示从任意位置开始定位 <code>headers = tree.xpath(&#39;//h1&#39;)  # // 表示从任意位置开始定位</code></li>
<li>属性定位： <code>//tagName[@attrName=&quot;attrValue&quot;]</code> ，比如 <code>disp1 = tree.xpath(&#39;//div[@class=&quot;disp-1&quot;]&#39;)</code> 获取到class 属性为 disp-1 的div元素</li>
<li>索引定位： <code>//tagName[@attrName=&quot;attrValue&quot;]/subTagName[index]</code> ，比如 <code>disp3 = tree.xpath(&#39;//body/div[3]&#39;)</code> 表示获得body标签下的第三个div元素</li>
<li>取出文本：<ul>
<li><code>/text()</code> 取出元素中<strong>直系</strong>的文本元素，比如 <code>disp1_text = tree.xpath(&#39;//div[3]/text()&#39;)</code> 取出了全局第3个div中的文字，返回值为一个列表，要想取得文本，应该去一个下标，也就是 <code>disp1_text = tree.xpath(&#39;//body/text()&#39;)[0] </code></li>
<li><code>//text()</code> 取出元素的所有文本元素，比如<code>disp_all_text = tree.xpath(&#39;//body//text()&#39;)</code></li>
</ul>
</li>
<li>取出标签属性：<ul>
<li><code>/@attrName</code> ，也会返回一个属性值列表，我们可以去下表来取得元素的属性值。比如<code>img_src = tree.xpath(&#39;//body/img/@src&#39;)[0]</code></li>
</ul>
</li>
</ul>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">title</span>&gt;</span>53<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">button</span>&gt;</span>替换节点<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">h1</span>&gt;</span>我是标题1<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">h1</span>&gt;</span>我是标题1<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span>&gt;</span>我是段落<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;disp-1&quot;</span>&gt;</span>this is disp1<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;disp-2&quot;</span>&gt;</span>this is disp2<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;disp-3&quot;</span>&gt;</span>this is disp3<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;https://hcdn2.luffycity.com/media/frontend/activity/head-logo_1564141048.3435316.svg&quot;</span>  <span class="attr">alt</span>=<span class="string">&quot;tupian&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>注意：所有xpath表达式中的下标都以1开头，所有的python列表都是以0开头</p>
<h4 id="案例一-解析58二手房的相关数据"><a href="#案例一-解析58二手房的相关数据" class="headerlink" title="案例一: 解析58二手房的相关数据"></a>案例一: 解析58二手房的相关数据</h4><blockquote>
<p>从<a target="_blank" rel="noopener" href="https://zz.58.com/ershoufang/">58二手房网站</a>中获取二手房的所有标题和价格</p>
</blockquote>
<p>注意：获取到了一个xpath节点之后，再以获取到的这个节点为根节点寻找的时候不能用<code>/tag/tag...</code> 而是应该用<code>./tag/..</code>， 比如 我们已经找到了 一个div节点，我们要再次寻找这个节点下的第二个div节点，因此我们需要使用<code>theDiv.xpath(&#39;./div[2]&#39;)</code></p>
<p>源码见 <code>Learning\Python\Spider\02-DataAnalysis\02-xpath-SecondhandHouse.py</code></p>
<h4 id="案例二：4K图片解析下载"><a href="#案例二：4K图片解析下载" class="headerlink" title="案例二：4K图片解析下载"></a>案例二：4K图片解析下载</h4><blockquote>
<p>从<a target="_blank" rel="noopener" href="http://pic.netbian.com/4kfengjing/">高清壁纸网</a>中爬取高清壁纸的缩略图</p>
</blockquote>
<p>中文乱码问题的通用解决方案：</p>
<p><code>img_name.encode(&#39;iso-8859-1&#39;).decode(&#39;gbk&#39;)</code></p>
<p>或者直接查看页面元数据的编码方式</p>
<p>创建文件夹和写入图片数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文件夹来保存图片</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;./picLibs&#x27;</span>):</span><br><span class="line">    os.mkdir(<span class="string">&#x27;./picLibs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将图片写入文件</span></span><br><span class="line">img_data = requests.get(url=total_url, headers=headers).content</span><br><span class="line">img_path = <span class="string">&#x27;picLibs/&#x27;</span> + pic_title</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(img_path, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(img_data)</span><br><span class="line">    <span class="built_in">print</span>(pic_title + <span class="string">&#x27; success&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>源码见 <code>Learning\Python\Spider\02-DataAnalysis\03-xpath-PictureDownload.py</code></p>
<h4 id="案例三：全国城市名称提取"><a href="#案例三：全国城市名称提取" class="headerlink" title="案例三：全国城市名称提取"></a>案例三：全国城市名称提取</h4><blockquote>
<p>从<a target="_blank" rel="noopener" href="https://www.aqistudy.cn/historydata/">中国天气网</a>爬取中国所有的城市的名称</p>
</blockquote>
<p>没有很难的知识点，解析略，源码见<code>Learning\Python\Spider\02-DataAnalysis\03-xpath-AllPath.py</code></p>
<h2 id="验证码识别"><a href="#验证码识别" class="headerlink" title="验证码识别"></a>验证码识别</h2><h3 id="使用工具类和api接口"><a href="#使用工具类和api接口" class="headerlink" title="使用工具类和api接口"></a>使用工具类和api接口</h3><p>步骤：</p>
<ul>
<li><p>将验证码事先下载到本地</p>
</li>
<li><p>将本地存储的验证码提交给平台的示例程序进行识别操作</p>
</li>
</ul>
<p>识别平台：<code>http://www.chaojiying.com/user/</code> </p>
<p>具体代码见<code>Learning\Python\Spider\03-VertifyCodeAnalysis\01-recognizeVertifyCode.py</code></p>
<h3 id="模拟登录-Cookie"><a href="#模拟登录-Cookie" class="headerlink" title="模拟登录-Cookie"></a>模拟登录-Cookie</h3><p>处理cookie的两种操作：</p>
<ul>
<li>手动处理，抓包之后封装到header中【不推荐】</li>
<li>自动处理</li>
</ul>
<p>http&#x2F;https协议的特性：无状态</p>
<p>同一个文件内，第一次请求登录成功之后，第二次请求登陆后的页面数据失败的原因：服务器不知道这次请求是基于登录状态下的请求。</p>
<p>cookie：用来让服务器记录客户端的相关状态，cookie 的值来自模拟登录post 请求后由服务器创建</p>
<p>session：作用: 发送请求、存储cookie</p>
<p>因此，我们需要创建一个session对象，用session对象进行模拟登录post请求的发送（cookie就会被存储在session中）</p>
<p>session对象对个人主页对应的get请求进行发送（携带了cookie）</p>
<hr>
<p>具体分析方法：</p>
<p>打开古诗文网的登录页面 <code>https://so.gushiwen.cn/user/login.aspx</code>,开启捕获数据包，输入密码登录。</p>
<p>可以看到html类型的post请求的数据包名为login，可以猜想就是我们要找的数据包</p>
<p>![[img&#x2F;image-20210128143417376.png]]<br>查看数据包的消息头、请求数据：</p>
<p>消息头</p>
<p>![[img&#x2F;image-20210128143552885.png]]<br>数据</p>
<p>![[img&#x2F;image-20210128143535267.png]]<br>因此，登录行为就是向消息头所在的地址发送表单数据中的数据包<code>page_data = session.post(url=login_url, headers=headers, data=data).text</code></p>
<p>然后获得返回的数据，就是我们想要的页面。</p>
<p>这里要注意一下，应该用同一个session访问，以进行cookie的留存。</p>
<p>具体代码见：<code>Learning\Python\Spider\03-VertifyCodeAnalysis\02-LoginGithub.py</code></p>
<h3 id="模拟登录-代理IP"><a href="#模拟登录-代理IP" class="headerlink" title="模拟登录-代理IP"></a>模拟登录-代理IP</h3><blockquote>
<p>这个要钱的，而且比较贵，先不学了吧。</p>
</blockquote>
<p>场景：多次请求同一个网站，网站的反爬机制组织我们再次请求这个网站，给我们返回错误结果403（服务器拒绝访问）</p>
<p>代理就是用来破解封IP这种反爬机制的。</p>
<p>代理：代理服务器，它的功能就是代理网络用户去取得网络信息。</p>
<p>作用：</p>
<ul>
<li>突破自身IP访问的限制，访问一些平时不能访问的站点。</li>
<li>隐藏真实IP，免受攻击，防止自身IP被封锁</li>
</ul>
<p>代理IP的类型：</p>
<ul>
<li>http：应用到http协议对应的url中</li>
<li>https：应用到https协议对应的url中</li>
</ul>
<p>代理IP的匿名度：</p>
<ul>
<li>透明：服务器知道这次请求使用了代理，也知道请求对应的真实ip</li>
<li>匿名：知道使用了代理，不知道真实ip</li>
<li>高匿：不知道使用了代理，更不知道真实ip</li>
</ul>
<h2 id="高性能异步爬虫"><a href="#高性能异步爬虫" class="headerlink" title="高性能异步爬虫"></a>高性能异步爬虫</h2><p>目的：在爬虫中使用异步实现高性能的数据爬取操作</p>
<p>方式：</p>
<ul>
<li>多线程，多进程（不建议使用）：<ul>
<li>好处：可以为相关阻塞操作开启线程或者进程</li>
<li>坏处：无法无限制的开启多线程或者多进程</li>
</ul>
</li>
<li>线程池、进程池（少用）：<ul>
<li>好处：我们可以降低系统对进程或者线程创建和销毁的一个频率，从而很好的降低系统的开销</li>
<li>弊端：池中线程或进程的数量有上限</li>
</ul>
</li>
<li>单线程异步协程（<strong>推荐</strong>）</li>
</ul>
<p>Pool线程池对象一定要放在main函数下面，不放在这里会报错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing.dummy <span class="keyword">import</span> Pool</span><br><span class="line"></span><br><span class="line"><span class="meta">... </span>...</span><br><span class="line"><span class="comment"># 实例化一个线程池对象</span></span><br><span class="line">pool = Pool(<span class="number">4</span>)  <span class="comment"># 表示线程池中有4个线程对象</span></span><br><span class="line"><span class="comment"># 第一个参数为阻塞函数，第二个为可迭代对象</span></span><br><span class="line"><span class="comment"># 函数执行的时候会开辟线程并把可迭代对象中的值依次传入到函数中进行执行</span></span><br><span class="line">pool.<span class="built_in">map</span>(get_page, name_list)</span><br></pre></td></tr></table></figure>

<p>单线程异步协程：</p>
<p>event_loop：事件循环，相当于一个无限循环，我们可以把一些函数注册到这个事件循环上，当满足某些条件的时候，函数就会被循环执行。</p>
<p>coroutine：协程对象，我们可以将协程对象注册到事件循环中，它会被事件循环调用。我们可以使用async 关键字来定义一个方法，这个方法在调用时不会立即被执行，而是返回一个协程对象。</p>
<p>task：任务，它是对协程对象的进一步封装，包含了任务的各个状态。</p>
<p>future：代表将来执行或还没有执行的任务，实际上和task 没有本质区别。</p>
<p>async：定义一个协程。</p>
<p>await：用来挂起阻塞方法的执行。</p>
<p>协程相关操作：</p>
<h3 id="案例一：爬取梨视频的视频数据"><a href="#案例一：爬取梨视频的视频数据" class="headerlink" title="案例一：爬取梨视频的视频数据"></a>案例一：爬取梨视频的视频数据</h3><p>在这个案例中，把写文件功能写成一个函数（因为这个功能比较耗时）。</p>
<p><strong>这个网站加上了反爬机制，无法破解，故没有做，直接粘贴的教程上的代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"><span class="comment">#安装fake-useragent库:pip install fake-useragent</span></span><br><span class="line"><span class="comment">#导入线程池模块</span></span><br><span class="line"><span class="keyword">from</span> multiprocessing.dummy <span class="keyword">import</span> Pool</span><br><span class="line"><span class="comment">#实例化线程池对象</span></span><br><span class="line">pool = Pool()</span><br><span class="line">url = <span class="string">&#x27;http://www.pearvideo.com/category_1&#x27;</span></span><br><span class="line"><span class="comment">#随机产生UA</span></span><br><span class="line">ua = UserAgent().random</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>:ua</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">#获取首页页面数据</span></span><br><span class="line">page_text = requests.get(url=url,headers=headers).text</span><br><span class="line"><span class="comment">#对获取的首页页面数据中的相关视频详情链接进行解析</span></span><br><span class="line">tree = etree.HTML(page_text)</span><br><span class="line">li_list = tree.xpath(<span class="string">&#x27;//div[@id=&quot;listvideoList&quot;]/ul/li&#x27;</span>)</span><br><span class="line">detail_urls = []<span class="comment">#存储二级页面的url</span></span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">    detail_url = <span class="string">&#x27;http://www.pearvideo.com/&#x27;</span>+li.xpath(<span class="string">&#x27;./div/a/@href&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    title = li.xpath(<span class="string">&#x27;.//div[@class=&quot;vervideo-title&quot;]/text()&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">    detail_urls.append(detail_url)</span><br><span class="line">vedio_urls = []<span class="comment">#存储视频的url</span></span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> detail_urls:</span><br><span class="line">    page_text = requests.get(url=url,headers=headers).text</span><br><span class="line">    vedio_url = re.findall(<span class="string">&#x27;srcUrl=&quot;(.*?)&quot;&#x27;</span>,page_text,re.S)[<span class="number">0</span>]</span><br><span class="line">    vedio_urls.append(vedio_url) </span><br><span class="line"><span class="comment">#使用线程池进行视频数据下载    </span></span><br><span class="line">func_request = <span class="keyword">lambda</span> link:requests.get(url=link,headers=headers).content</span><br><span class="line">video_data_list = pool.<span class="built_in">map</span>(func_request,vedio_urls)</span><br><span class="line"><span class="comment">#使用线程池进行视频数据保存</span></span><br><span class="line">func_saveData = <span class="keyword">lambda</span> data:save(data)</span><br><span class="line">pool.<span class="built_in">map</span>(func_saveData,video_data_list)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">data</span>):</span><br><span class="line">    fileName = <span class="built_in">str</span>(random.randint(<span class="number">1</span>,<span class="number">10000</span>))+<span class="string">&#x27;.mp4&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fileName,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(data)</span><br><span class="line">        <span class="built_in">print</span>(fileName+<span class="string">&#x27;已存储&#x27;</span>)</span><br><span class="line">pool.close()</span><br><span class="line">pool.join()</span><br></pre></td></tr></table></figure>

<h2 id="Selenium"><a href="#Selenium" class="headerlink" title="Selenium"></a>Selenium</h2><h3 id="安装与环境测试"><a href="#安装与环境测试" class="headerlink" title="安装与环境测试"></a>安装与环境测试</h3><blockquote>
<p>源码：GITHUB\Learning\Python\Spider\05-Selenium\01-test.py</p>
</blockquote>
<p>安装环境 <code>pip install selenium</code></p>
<p>下载与浏览器对应的驱动，注意查看映射关系</p>
<p>使用步骤：</p>
<ul>
<li>实例化浏览器对象</li>
<li>编写自动化操作代码</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment">#  实例化一个浏览器对象</span></span><br><span class="line">    bro = webdriver.Firefox(executable_path=<span class="string">&#x27;./../Utils/geckodriver.exe&#x27;</span>)</span><br><span class="line">    <span class="comment">#  让浏览器发起一个指定的url对应请求 , python程序会自动调用驱动打开浏览器进行一系列操作</span></span><br><span class="line">    bro.get(<span class="string">&#x27;http://scxk.nmpa.gov.cn:81/xk/&#x27;</span>)</span><br><span class="line">    <span class="comment">#  获取页面源码</span></span><br><span class="line">    page_text = bro.page_source</span><br><span class="line">    tree = etree.HTML(page_text)</span><br><span class="line">    <span class="comment">#  获取列表块</span></span><br><span class="line">    list_block = tree.xpath(<span class="string">&#x27;//ul[@id=&quot;gzlist&quot;]/li&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> items <span class="keyword">in</span> list_block:</span><br><span class="line">        title = items.xpath(<span class="string">&#x27;./dl/@title&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="built_in">print</span>(title)</span><br><span class="line">    <span class="comment">#  关闭浏览器</span></span><br><span class="line">    bro.quit()</span><br></pre></td></tr></table></figure>

<h3 id="一些操作"><a href="#一些操作" class="headerlink" title="一些操作"></a>一些操作</h3><blockquote>
<p>源码：GITHUB\Learning\Python\Spider\05-Selenium\02-OtherAutoOperations.py</p>
</blockquote>
<ul>
<li>发起请求 <code>get(url)</code></li>
<li>标签定位 <code>find_...系列的方法</code> </li>
<li>标签交互 <code>send_keys(&#39;xxx&#39;)</code></li>
<li>执行js程序 <code>execute_sctript(&#39;JsCode&#39;)</code></li>
<li>前进和后退 <code>back(), forward()</code></li>
<li>关闭浏览器 <code>quit()</code></li>
</ul>
<p>定位元素：</p>
<p>![[img&#x2F;image-20210129212959044.png]]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 注册浏览器</span></span><br><span class="line">    bro = webdriver.Firefox(executable_path=<span class="string">&#x27;./../Utils/geckodriver.exe&#x27;</span>)</span><br><span class="line">    <span class="comment"># 获取页面</span></span><br><span class="line">    bro.get(<span class="string">&#x27;http://scxk.nmpa.gov.cn:81/xk/&#x27;</span>)</span><br><span class="line">    <span class="comment"># 标签定位 - 查询信息</span></span><br><span class="line">    enter = bro.find_element_by_id(<span class="string">&quot;searchtext&quot;</span>)</span><br><span class="line">    <span class="comment"># 向文本框中输入 20170002</span></span><br><span class="line">    enter.send_keys(<span class="string">&quot;20170002&quot;</span>)</span><br><span class="line">    <span class="comment">#  查找搜索按钮</span></span><br><span class="line">    btn = bro.find_element_by_id(<span class="string">&#x27;searchInfo&#x27;</span>)</span><br><span class="line">    <span class="comment">#  点击查找按钮按钮</span></span><br><span class="line">    btn.click()</span><br><span class="line">    <span class="comment">#  执行js代码，向下滚动一屏幕的距离</span></span><br><span class="line">    bro.execute_script(<span class="string">&#x27;window.scrollTo(0, document.body.scrollHeight)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 再次跳转到页面</span></span><br><span class="line">    bro.get(<span class="string">&#x27;http://kaifa.baidu.com/home&#x27;</span>)</span><br><span class="line">    <span class="comment"># 回退到上一个页面</span></span><br><span class="line">    bro.back()</span><br><span class="line">    <span class="comment"># 返回到上一个页面</span></span><br><span class="line">    bro.forward()</span><br><span class="line">    <span class="comment">#  关闭浏览器</span></span><br><span class="line">    bro.quit()</span><br></pre></td></tr></table></figure>

<h3 id="selenuim处理iframe"><a href="#selenuim处理iframe" class="headerlink" title="selenuim处理iframe"></a>selenuim处理iframe</h3><p>实现拖动操作（动作链）</p>
<blockquote>
<p>源码：GITHUB\Learning\Python\Spider\05-Selenium\03-ActionChain.py</p>
</blockquote>
<p>如果定位的标签在iframe中，则必须使用<code>switch_to.frame(id)</code></p>
<p>动作链：</p>
<ul>
<li><code>from selenium.webdriver import ActionChains</code></li>
<li>实例化一个动作链对象：<code>action = ActionChains(bro)</code></li>
<li><code>click_and_hold(div)</code> 长按且点击操作</li>
<li><code>move_by_offset(x, y)</code> 偏移</li>
<li><code>perform()</code> 立即执行动作链</li>
<li><code> action.release()</code> 释放动作链</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    bro = webdriver.Firefox(executable_path=<span class="string">&#x27;./../Utils/geckodriver.exe&#x27;</span>)</span><br><span class="line">    bro.get(<span class="string">&#x27;https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  如果要丁文的标签是iframe标签的话，就必须通过以下方式来转换定位作用域</span></span><br><span class="line">    <span class="comment">#  因为iframe是在html中应用的另一个html，定位的时候是默认的外层的html</span></span><br><span class="line">    bro.switch_to.frame(<span class="string">&#x27;iframeResult&#x27;</span>)</span><br><span class="line">    div = bro.find_element_by_id(<span class="string">&#x27;draggable&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 动作链</span></span><br><span class="line">    action = ActionChains(bro)</span><br><span class="line">    <span class="comment"># 点击长安指定的标签</span></span><br><span class="line">    action.click_and_hold(div)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模拟人拖动滑块</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        <span class="comment"># perform() 立即执行动作链</span></span><br><span class="line">        <span class="comment"># move_by_offset(x, y) 偏移</span></span><br><span class="line">        action.move_by_offset(<span class="number">17</span>, <span class="number">0</span>).perform()</span><br><span class="line">        <span class="comment"># 休眠0.3秒</span></span><br><span class="line">        sleep(<span class="number">0.1</span>)</span><br><span class="line">    <span class="comment"># 释放动作链</span></span><br><span class="line">    action.release()</span><br><span class="line">    sleep(<span class="number">5</span>)</span><br><span class="line">    bro.quit()</span><br></pre></td></tr></table></figure>


<h3 id="无头浏览器-规避检测"><a href="#无头浏览器-规避检测" class="headerlink" title="无头浏览器 + 规避检测"></a>无头浏览器 + 规避检测</h3><blockquote>
<p>Learning\Python\Spider\05-Selenium\04-HeadlessBrowser.py</p>
</blockquote>
<p>无可视化界面的浏览器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> sleep</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.firefox.options <span class="keyword">import</span> Options</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    firefox_options = Options()</span><br><span class="line">    firefox_options.add_argument(<span class="string">&#x27;--headless&#x27;</span>)</span><br><span class="line">    firefox_options.add_argument(<span class="string">&#x27;--disable-gpu&#x27;</span>)</span><br><span class="line">    bro = webdriver.Firefox(executable_path=<span class="string">&#x27;./../Utils/geckodriver.exe&#x27;</span>, firefox_options=firefox_options)</span><br><span class="line">    <span class="comment"># ------------无头浏览器-----------</span></span><br><span class="line">    bro.get(<span class="string">&quot;http://kaifa.baidu.com/home&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(bro.page_source)</span><br><span class="line">    bro.quit()</span><br></pre></td></tr></table></figure>

<p>规避检测，用的时候直接复制就行</p>
<p>火狐的有问题，这个是chrome的 ，火狐的没有add_experimental_option对象实例，所以先用chrome的吧</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="comment"># 实现无可视化界面</span></span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line"><span class="comment"># 实现规避建测</span></span><br><span class="line"><span class="keyword">from</span> selenium.webdriver <span class="keyword">import</span> ChromeOptions</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 实现无可视化操作</span></span><br><span class="line">    chrome_options = Options()</span><br><span class="line">    chrome_options.add_argument(<span class="string">&#x27;--headless&#x27;</span>)</span><br><span class="line">    chrome_options.add_argument(<span class="string">&#x27;--disable-gpu&#x27;</span>)</span><br><span class="line">    <span class="comment"># 实现规避检测</span></span><br><span class="line">    options = ChromeOptions()</span><br><span class="line">    options.add_experimental_option(<span class="string">&#x27;excludeSwitches&#x27;</span>, [<span class="string">&#x27;enable-automation&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    bro = webdriver.Firefox(executable_path=<span class="string">&#x27;./driver&#x27;</span>,</span><br><span class="line">                            firefox_options=chrome_options,</span><br><span class="line">                            options=options)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ------------无头浏览器 + 规避检测-----------</span></span><br><span class="line">    bro.get(<span class="string">&quot;http://kaifa.baidu.com/home&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(bro.page_source)</span><br><span class="line">    bro.quit()</span><br></pre></td></tr></table></figure>

<h2 id="Scrapy框架"><a href="#Scrapy框架" class="headerlink" title="Scrapy框架"></a>Scrapy框架</h2><h3 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h3><p>是什么？ 爬虫中封装好的一个明星框架，功能: 高性能的持久化存储，高性能的数据解析，分布式处理</p>
<p>安装：max &#x2F; linux <code>pip install scrapy</code></p>
<p>windows安装</p>
<ul>
<li><code>pip install wheel</code></li>
<li>下载twisted 下载地址为<a target="_blank" rel="noopener" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted">http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</a>  下载文件<code>ad3‑2.2.1‑cp37‑cp37m‑win_amd64.whl</code> cp37表示python3.7，win_amd64表示win64</li>
<li>在有这个轮子的资源管理器页面打开cmd执行<code>pip install Twisted‑17.1.0‑cp36‑cp36m‑win_amd64.whl</code></li>
<li><code>pip install pywin32</code></li>
<li><code>pip install scrapy</code></li>
</ul>
<p>测试：在终端里录入scrapy指令，没有报错即表示安装成功！</p>
<p>scrapy使用流程：</p>
<ul>
<li>创建工程：<code>scrapy startproject ProName</code></li>
<li>进入工程目录：<code>cd ProName</code></li>
<li>创建爬虫文件：<code>scrapy genspider spiderName www.xxx.com</code></li>
<li>编写相关操作代码</li>
<li>执行工程：<code>scrapy crawl spiderName</code></li>
</ul>
<p>scrapy在执行的时候会有很多日志信息，如果不想看，可以在配置文件<code>project/settings.py</code> 中第23行附近加上这句话<code>LOG_LEVEL = &#39;ERROR&#39; # 只显示指定类型的日志信息</code></p>
<p>请求头的改变：settings.py中，17行的useragent注释解开，然后加上自己的请求头即可</p>
<h3 id="Scrapy实现数据解析与数据持久化"><a href="#Scrapy实现数据解析与数据持久化" class="headerlink" title="Scrapy实现数据解析与数据持久化"></a>Scrapy实现数据解析与数据持久化</h3><p><code>scrapy genspider spiderName www.xxx.com</code> 创建爬虫文件之后，会在spider中自动生成一个爬虫文件，其中的parse方法就是用来处理数据的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">QiubaiSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;qiubai&#x27;</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.qiushibaike.com/text/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        div_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;col1 old-style-col1&quot;]/div&#x27;</span>)</span><br><span class="line">        all_data = []</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            <span class="comment"># xpath返回的是列表，返回的是Selector类型对象</span></span><br><span class="line">            <span class="comment"># 使用 extract() 提取data中的字符串</span></span><br><span class="line">            title = div.xpath(<span class="string">&#x27;./div[1]/a[2]/h2/text()&#x27;</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            <span class="comment"># 列表调用了extract表示将每一个select对象中的data字符串提取出来，返回列表</span></span><br><span class="line">            content = div.xpath(<span class="string">&#x27;./a[1]/div/span/text()&#x27;</span>).extract()</span><br><span class="line">            content = <span class="string">&#x27;&#x27;</span>.join(content)</span><br><span class="line">            all_data.append(&#123;</span><br><span class="line">                <span class="string">&quot;author&quot;</span>: title,</span><br><span class="line">                <span class="string">&quot;content&quot;</span>: content</span><br><span class="line">            &#125;)</span><br><span class="line">        <span class="keyword">return</span> all_data</span><br></pre></td></tr></table></figure>

<hr>
<p>持久化方法：</p>
<ul>
<li>基于终端指令的持久化<ul>
<li>只能持久化到文件</li>
<li>只能将返回值持久化，我们要对清洗后的数据进行封装</li>
<li>高效便捷，但局限性强（文件类型有限）</li>
</ul>
</li>
<li>基于管道的持久化</li>
</ul>
<p>基于终端指令的持久化：</p>
<p>爬虫文件中的parse方法必须有返回值</p>
<p><code>scrapy crawl 爬虫名称 -o 路径</code></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl 爬虫名称 -o xxx.json</span><br><span class="line">scrapy crawl 爬虫名称 -o xxx.xml</span><br><span class="line">scrapy crawl 爬虫名称 -o xxx.csv</span><br></pre></td></tr></table></figure>

<p>举例<code>D:\GITHUB\Learning\Python\Spider\06-Scrapy\QiuBai&gt;scrapy crawl qiubai -o ./qiubai.csv</code></p>
<hr>
<p>基于管道的持久化，流程：</p>
<ol>
<li>数据解析</li>
<li>在item类中定义相关的属性,  实现<code>items.py</code>, 属性格式：<code>name = scrapy.Field()</code></li>
<li>将解析的数据封装存储到item类型的对象中</li>
<li>将item类型的对象提交给管道进行持久化存储的操作 <code>piplines.py</code></li>
<li>在管道类的ptocess_item中接收爬虫文件提交过来的item对象，然后编写持久化存储的代码将item对象中存储的数据进行持久化存储</li>
<li><code>settings.py</code>配置文件中开启管道</li>
</ol>
<p>源码见：<code>GITHUB\Learning\Python\Spider\06-Scrapy\BasedOnPipline</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  spider.py</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="comment">#  注意，这里在导包的时候要将项目的目录mark为Source Root</span></span><br><span class="line"><span class="keyword">from</span> BasedOnPipline.items <span class="keyword">import</span> BasedonpiplineItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QiubaiSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;BasedOnPipline&#x27;</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.qiushibaike.com/text/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        div_list = response.xpath(<span class="string">&#x27;//div[@class=&quot;col1 old-style-col1&quot;]/div&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> div <span class="keyword">in</span> div_list:</span><br><span class="line">            <span class="comment"># xpath返回的是列表，返回的是Selector类型对象</span></span><br><span class="line">            <span class="comment"># 使用 extract() 提取data中的字符串</span></span><br><span class="line">            title = div.xpath(<span class="string">&#x27;./div[1]/a[2]/h2/text() | ./div[1]/span/h2/text()&#x27;</span>)[<span class="number">0</span>].extract()</span><br><span class="line">            <span class="comment"># 列表调用了extract表示将每一个select对象中的data字符串提取出来，返回列表</span></span><br><span class="line">            content = div.xpath(<span class="string">&#x27;./a[1]/div/span/text()&#x27;</span>).extract()</span><br><span class="line">            content = <span class="string">&#x27;&#x27;</span>.join(content)</span><br><span class="line">            item = BasedonpiplineItem()</span><br><span class="line">            item[<span class="string">&#x27;author&#x27;</span>] = title</span><br><span class="line">            item[<span class="string">&#x27;content&#x27;</span>] = content</span><br><span class="line">            <span class="keyword">yield</span> item  <span class="comment"># 提交item</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># items.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasedonpiplineItem</span>(scrapy.Item):</span><br><span class="line">    author = scrapy.Field()</span><br><span class="line">    content = scrapy.Field()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># puplines.py</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasedonpiplinePipeline</span>:</span><br><span class="line">    fp = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重写父类的一个方法没这份方法只在开始爬虫的时候调用一次</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;start.....&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fp = <span class="built_in">open</span>(<span class="string">&#x27;./qiubai.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用来处理item类型的对象的</span></span><br><span class="line">    <span class="comment"># 该方法可以接受爬虫文件提交的item对象</span></span><br><span class="line">    <span class="comment"># 每接受一次参数都会调用一次</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        author = item[<span class="string">&#x27;author&#x27;</span>]</span><br><span class="line">        content = item[<span class="string">&#x27;content&#x27;</span>]</span><br><span class="line">        <span class="variable language_">self</span>.fp.write(author + <span class="string">&#x27; : &#x27;</span> + content + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;the end...&#x27;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.fp.close()</span><br></pre></td></tr></table></figure>

<p>注意：</p>
<ul>
<li>settings.py 里面要设置userAgent 和 ITEM_PIPELINES</li>
</ul>
<p>面试题：</p>
<p>如果最终需要将爬取到的数据值一份存储到磁盘文件，一份存储到数据库中，则应该如何操作scrapy？</p>
<p>答：管道文件中的代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#该类为管道类，该类中的process_item方法是用来实现持久化存储操作的。</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DoublekillPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">	<span class="comment">#持久化操作代码 （方式1：写入磁盘文件）</span></span><br><span class="line">	<span class="keyword">return</span> item</span><br><span class="line">    <span class="comment">#如果想实现另一种形式的持久化操作，则可以再定制一个管道类：</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DoublekillPipeline_db</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">    <span class="comment">#持久化操作代码 （方式2：写入数据库）</span></span><br><span class="line">    <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<p>settings.py 中开启管道操作的代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下列结构为字典，字典中的键值表示的是即将被启用执行的管道文件和其执行的优先级。</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line"> <span class="string">&#x27;doublekill.pipelines.DoublekillPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">  <span class="string">&#x27;doublekill.pipelines.DoublekillPipeline_db&#x27;</span>: <span class="number">200</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#上述代码中，字典中的两组键值分别表示会执行管道文件中对应的两个管道类中的process_item方法，实现两种不同形式的持久化操作。</span></span><br></pre></td></tr></table></figure>

<h4 id="练习-基于Spider的全站数据爬取"><a href="#练习-基于Spider的全站数据爬取" class="headerlink" title="练习 - 基于Spider的全站数据爬取"></a>练习 - 基于Spider的全站数据爬取</h4><p>题目：爬取小说 <a target="_blank" rel="noopener" href="http://www.paoshuzw.com/27/27256/">临渊行</a>  的所有章节内容</p>
<p>源码见 &#96;GITHUB\Learning\Python\Spider\06-Scrapy\Alldata</p>
<h3 id="Scrapy五大核心组件"><a href="#Scrapy五大核心组件" class="headerlink" title="Scrapy五大核心组件"></a>Scrapy五大核心组件</h3><p>![[img&#x2F;image-20210208170904800.png]]</p>
<ul>
<li>引擎(Scrapy)<ul>
<li>用来处理整个系统的数据流处理, 触发事务(框架核心)</li>
</ul>
</li>
<li>调度器(Scheduler)<ul>
<li>用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</li>
</ul>
</li>
<li>下载器(Downloader)<ul>
<li>用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)</li>
</ul>
</li>
<li>爬虫(Spiders)<ul>
<li>爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</li>
</ul>
</li>
<li>项目管道(Pipeline)<ul>
<li>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li>
</ul>
</li>
</ul>
<h3 id="请求传参"><a href="#请求传参" class="headerlink" title="请求传参"></a>请求传参</h3><ul>
<li>在某些情况下，我们爬取的数据不在同一个页面中，例如，我们爬取一个电影网站，电影的名称，评分在一级页面，而要爬取的其他电影详情在其二级子页面中。这时我们就需要用到请求传参。</li>
<li>请求传参的使用场景<ul>
<li>当我们使用爬虫爬取的数据没有存在于同一张页面的时候，则必须使用请求传参</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> bossPro.items <span class="keyword">import</span> BossproItem</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BossSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;boss&#x27;</span></span><br><span class="line">    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]</span></span><br><span class="line">    start_urls = [<span class="string">&#x27;https://www.zhipin.com/job_detail/?query=python&amp;city=101010100&amp;industry=&amp;position=&#x27;</span>]</span><br><span class="line">    url = <span class="string">&#x27;https://www.zhipin.com/c101010100/?query=python&amp;page=%d&#x27;</span></span><br><span class="line">    page_num = <span class="number">2</span></span><br><span class="line">   <span class="comment">#回调函数接受item</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_detail</span>(<span class="params">self,response</span>):</span><br><span class="line">        item = response.meta[<span class="string">&#x27;item&#x27;</span>]</span><br><span class="line">        job_desc = response.xpath(<span class="string">&#x27;//*[@id=&quot;main&quot;]/div[3]/div/div[2]/div[2]/div[1]/div//text()&#x27;</span>).extract()</span><br><span class="line">        job_desc = <span class="string">&#x27;&#x27;</span>.join(job_desc)</span><br><span class="line">        <span class="comment"># print(job_desc)</span></span><br><span class="line">        item[<span class="string">&#x27;job_desc&#x27;</span>] = job_desc</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line">    <span class="comment">#解析首页中的岗位名称</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        li_list = response.xpath(<span class="string">&#x27;//*[@id=&quot;main&quot;]/div/div[3]/ul/li&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">            item = BossproItem()</span><br><span class="line">            job_name = li.xpath(<span class="string">&#x27;.//div[@class=&quot;info-primary&quot;]/h3/a/div[1]/text()&#x27;</span>).extract_first()</span><br><span class="line">            item[<span class="string">&#x27;job_name&#x27;</span>] = job_name</span><br><span class="line">            <span class="comment"># print(job_name)</span></span><br><span class="line">            detail_url = <span class="string">&#x27;https://www.zhipin.com&#x27;</span>+li.xpath(<span class="string">&#x27;.//div[@class=&quot;info-primary&quot;]/h3/a/@href&#x27;</span>).extract_first()</span><br><span class="line">            <span class="comment">#对详情页发请求获取详情页的页面源码数据</span></span><br><span class="line">            <span class="comment">#手动请求的发送</span></span><br><span class="line">            <span class="comment">#请求传参：meta=&#123;&#125;，可以将meta字典传递给请求对应的回调函数</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(detail_url,callback=<span class="variable language_">self</span>.parse_detail,meta=&#123;<span class="string">&#x27;item&#x27;</span>:item&#125;)</span><br><span class="line">        <span class="comment">#分页操作</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.page_num &lt;= <span class="number">3</span>:</span><br><span class="line">            new_url = <span class="built_in">format</span>(<span class="variable language_">self</span>.url%<span class="variable language_">self</span>.page_num)</span><br><span class="line">            <span class="variable language_">self</span>.page_num += <span class="number">1</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(new_url,callback=<span class="variable language_">self</span>.parse)</span><br></pre></td></tr></table></figure>

<ul>
<li>增加并发：<ul>
<li>默认scrapy开启的并发线程为32个，可以适当进行增加。在settings配置文件中修改CONCURRENT_REQUESTS &#x3D; 100值为100,并发设置成了为100。</li>
</ul>
</li>
<li>降低日志级别：<ul>
<li>在运行scrapy时，会有大量日志信息的输出，为了减少CPU的使用率。可以设置log输出信息为INFO或者ERROR即可。在配置文件中编写：LOG_LEVEL &#x3D; ‘INFO’</li>
</ul>
</li>
<li>禁止cookie：<ul>
<li>如果不是真的需要cookie，则在scrapy爬取数据时可以禁止cookie从而减少CPU的使用率，提升爬取效率。在配置文件中编写：COOKIES_ENABLED &#x3D; False</li>
</ul>
</li>
<li>禁止重试：<ul>
<li>对失败的HTTP进行重新请求（重试）会减慢爬取速度，因此可以禁止重试。在配置文件中编写：RETRY_ENABLED &#x3D; False</li>
</ul>
</li>
<li>减少下载超时：<ul>
<li>如果对一个非常慢的链接进行爬取，减少下载超时可以能让卡住的链接快速被放弃，从而提升效率。在配置文件中进行编写：DOWNLOAD_TIMEOUT &#x3D; 10 超时时间为10s</li>
</ul>
</li>
</ul>
<h3 id="图片数据爬取"><a href="#图片数据爬取" class="headerlink" title="图片数据爬取"></a>图片数据爬取</h3><ul>
<li>在scrapy中我们之前爬取的都是基于字符串类型的数据，那么要是基于图片数据的爬取，那又该如何呢？<ul>
<li>其实在scrapy中已经为我们封装好了一个专门基于图片请求和持久化存储的管道类ImagesPipeline，那也就是说如果想要基于scrapy实现图片数据的爬取，则可以直接使用该管道类即可。</li>
</ul>
</li>
<li>ImagesPipeline使用流程<ul>
<li>在配置文件中进行如下配置：<br>IMAGES_STORE &#x3D; ‘.&#x2F;imgs’：表示最终图片存储的目录</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pipline.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line">  <span class="keyword">import</span> scrapy</span><br><span class="line">  <span class="keyword">class</span> <span class="title class_">ImgproPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">      item = <span class="literal">None</span></span><br><span class="line">      <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">          <span class="comment"># print(item)</span></span><br><span class="line">          <span class="keyword">return</span> item</span><br><span class="line">  <span class="comment">#ImagesPipeline专门用于文件下载的管道类，下载过程支持异步和多线程</span></span><br><span class="line">  <span class="keyword">class</span> <span class="title class_">ImgPipeLine</span>(<span class="title class_ inherited__">ImagesPipeline</span>):</span><br><span class="line">      <span class="comment">#对item中的图片进行请求操作</span></span><br><span class="line">      <span class="keyword">def</span> <span class="title function_">get_media_requests</span>(<span class="params">self, item, info</span>):</span><br><span class="line">          <span class="keyword">yield</span> scrapy.Request(item[<span class="string">&#x27;src&#x27;</span>])</span><br><span class="line">      <span class="comment">#定制图片的名称</span></span><br><span class="line">      <span class="keyword">def</span> <span class="title function_">file_path</span>(<span class="params">self, request, response=<span class="literal">None</span>, info=<span class="literal">None</span></span>):</span><br><span class="line">          url = request.url</span><br><span class="line">          file_name = url.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>]</span><br><span class="line">          <span class="keyword">return</span> file_name</span><br><span class="line">      <span class="keyword">def</span> <span class="title function_">item_completed</span>(<span class="params">self, results, item, info</span>):</span><br><span class="line">          <span class="keyword">return</span> item  <span class="comment">#该返回值会传递给下一个即将被执行的管道类</span></span><br></pre></td></tr></table></figure>

<h3 id="selenium的应用"><a href="#selenium的应用" class="headerlink" title="selenium的应用"></a>selenium的应用</h3><ul>
<li><p>在通过scrapy框架进行某些网站数据爬取的时候，往往会碰到页面动态数据加载的情况发生，如果直接使用scrapy对其url发请求，是绝对获取不到那部分动态加载出来的数据值。但是通过观察我们会发现，通过浏览器进行url请求发送则会加载出对应的动态加载出的数据。那么如果我们想要在scrapy也获取动态加载出的数据，则必须使用selenium创建浏览器对象，然后通过该浏览器对象进行请求发送，获取动态加载的数据值。</p>
<h4 id="案例分析："><a href="#案例分析：" class="headerlink" title="案例分析："></a>案例分析：</h4></li>
<li><p>需求：爬取网易新闻的国内板块下的新闻数据</p>
</li>
<li><p>需求分析：当点击国内超链进入国内对应的页面时，会发现当前页面展示的新闻数据是被动态加载出来的，如果直接通过程序对url进行请求，是获取不到动态加载出的新闻数据的。则就需要我们使用selenium实例化一个浏览器对象，在该对象中进行url的请求，获取动态加载的新闻数据。</p>
<h4 id="selenium在scrapy中使用的原理分析："><a href="#selenium在scrapy中使用的原理分析：" class="headerlink" title="selenium在scrapy中使用的原理分析："></a>selenium在scrapy中使用的原理分析：</h4></li>
<li><p>当引擎将国内板块url对应的请求提交给下载器后，下载器进行网页数据的下载，然后将下载到的页面数据，封装到response中，提交给引擎，引擎将response在转交给Spiders。Spiders接受到的response对象中存储的页面数据里是没有动态加载的新闻数据的。要想获取动态加载的新闻数据，则需要在下载中间件中对下载器提交给引擎的response响应对象进行拦截，切对其内部存储的页面数据进行篡改，修改成携带了动态加载出的新闻数据，然后将被篡改的response对象最终交给Spiders进行解析操作。</p>
<h4 id="selenium在scrapy中的使用流程："><a href="#selenium在scrapy中的使用流程：" class="headerlink" title="selenium在scrapy中的使用流程："></a>selenium在scrapy中的使用流程：</h4></li>
<li><p>重写爬虫文件的构造方法，在该方法中使用selenium实例化一个浏览器对象（因为浏览器对象只需要被实例化一次）</p>
</li>
<li><p>重写爬虫文件的closed(self,spider)方法，在其内部关闭浏览器对象。该方法是在爬虫结束时被调用</p>
</li>
<li><p>重写下载中间件的process_response方法，让该方法对响应对象进行拦截，并篡改response中存储的页面数据</p>
</li>
<li><p>在配置文件中开启下载中间件</p>
</li>
</ul>
</div>
    </div>

    <div class="post-meta">
        <i>
        
            <span>2021-01-26</span>
            
            
             
        
        </i>
    </div>
    <br>
    
    
        
            
    
            <div class="post-footer-pre-next">
                
                    <span>上一篇：<a href='/2021/02/20/ES6-11/'>笔记 | ES6-11新特性</a></span>
                

                
                    <span class="post-footer-pre-next-last-span-right">下一篇：<a href="/2021/01/11/SpringMVC/">笔记 | SpringMVC</a>
                    </span>
                
            </div>
    
        
    

    
        

     
</div>



                                      
                    
                    
                    <div class="footer">
    
        <span> 
             

            
                

            
        </span>
       
    
</div>



<!--这是指一条线往下的内容-->
<div class="footer-last">
    
            <span>Change And Challenge</span>
            
    
</div>


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>

    <!--目录-->
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.7.2/jquery.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tocify/1.9.0/javascripts/jquery.tocify.min.js" type="text/javascript" ></script>
        
<script src="/js/toc.js"></script>

    

    
<script src="/js/randomHeaderContent.js"></script>

    <!--回到顶部按钮-->
    
        
<script src="/js/returnToTop.js"></script>

    

    
        
<script src="/js/returnToLastPage.js"></script>

    





<script src="/js/lightgallery/lightgallery.umd.min.js"></script>



<script src="/js/lightgallery/plugins/lg-thumbnail.umd.min.js"></script>



<script src="/js/lightgallery/plugins/lg-fullscreen.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-autoplay.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-zoom.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-rotate.umd.min.js"></script>


<script src="/js/lightgallery/plugins/lg-paper.umd.min.js"></script>




<script type="text/javascript">
     
    if (typeof lightGallery !== "undefined") {
        var options1 = {
            selector: '.gallery-item',
            plugins: [lgThumbnail, lgFullscreen, lgAutoplay, lgZoom, lgRotate, lgPager], // 启用插件
            thumbnail: true,          // 显示缩略图
            zoom: true,               // 启用缩放功
            rotate: true,             // 启用旋转功能能
            autoplay: true,        // 启用自动播放功能
            fullScreen: true,      // 启用全屏功能
            pager: false, //页码,
            zoomFromOrigin: true,   // 从原始位置缩放
            actualSize: true,       // 启用查看实际大小的功能
            enableZoomAfter: 300,    // 延迟缩放，确保图片加载完成后可缩放
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options1); // 修复选择器
    }
    
</script>


                </div>
            
            
                <!-- 回到顶部的按钮-->  
                <div class="progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
            
                <!-- 返回的按钮-->  
                <div class="return-to-last-progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
    </body>
</html>